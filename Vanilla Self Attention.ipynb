{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History\n",
    "from keras import models, layers\n",
    "from keras import backend as K\n",
    "from skimage.transform import resize\n",
    "from keras.optimizers import Adam\n",
    "import json\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D, Layer, Lambda\n",
    "\n",
    "import warnings\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Tuple\n",
    "from keras.preprocessing.image import load_img\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Size of Image\n",
    "\n",
    "We define the size of the image, along with a scaling factor (in this case, default is `4`). The scaling factor plays a significant role in reducing the amount of computation, and weight updates, that the subsequent models must perform. They represent a tradeoff between model performance (richness of the feature set) and practical efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = int(480/8)\n",
    "HEIGHT = int(640/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Get Mappings Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two functions, `get_mappings` and `expand_tensor_shape`, that serve as helper functions for almost all test workflows. \n",
    "\n",
    "The `get_mappings` function is used to retrieve the mapping (a dictionary with keys as image filenames and values being their true class (ie. `male` or `smiling`). \n",
    "\n",
    "The `expand_tensor_shape` function is used to add an additional dimension at the end of the tensor. So for instance, a `40 x 50 x 3` tensor would be outputted as `40 x 50 x 3 x 1` after being passed into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(bucket_name: str, mapping_path: str) -> Dict[str, str]:\n",
    "    s3 = boto3.resource('s3')\n",
    "    faces_bucket = s3.Bucket(bucket_name)  # instantiate the bucket object\n",
    "\n",
    "    obj = s3.Object(bucket_name, mapping_path)  # fetch the mapping dictionary\n",
    "\n",
    "    json_string: str = obj.get()['Body'].read().decode('utf-8')\n",
    "    mappings_dict: Dict[str, str] = json.loads(\n",
    "        json_string)  # this mappings_dict contains filename -> gender class mapping\n",
    "    print(list(mappings_dict.items())[:3])  # print the first three entries of the mappings dictionary\n",
    "    return mappings_dict\n",
    "\n",
    "\n",
    "def expand_tensor_shape(X_train: np.ndarray) -> np.ndarray:\n",
    "    new_shape: Tuple = X_train.shape + (1,)\n",
    "    new_tensor = X_train.reshape(new_shape)\n",
    "    print(f\"Expanding shape from {X_train.shape} to {new_tensor.shape}\")\n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is /home/ubuntu/attention-facial-recognition\n",
      "[('1-01.jpg', 'male'), ('1-02.jpg', 'male'), ('1-03.jpg', 'male')]\n",
      "Downloading 1-11.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/1-11.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 1-11.jpg\n",
      "Downloading 1-12.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/1-12.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 1-12.jpg\n",
      "Downloading 1-13.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/1-13.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 1-13.jpg\n",
      "Downloading 10-02.jpg, saving as /home/ubuntu/attention-facial-recognition/faces/10-02.jpg\n",
      "An error occurred (404) when calling the HeadObject operation: Not Found:Error downloading 10-02.jpg\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "warnings.filterwarnings('ignore')\n",
    "S3_BUCKET_NAME = \"fei-faces-sao-paulo\"\n",
    "mapping = 'classification/gender.json'\n",
    "\n",
    "print(f\"Current working directory is {cwd}\")\n",
    "s3 = boto3.client('s3')\n",
    "warnings.filterwarnings('ignore')\n",
    "IMAGE_LIMIT = 3000\n",
    "LOCAL_IMAGES_FOLDER = \"faces\"\n",
    "\n",
    "mappings_dict: Dict[str, str] = get_mappings(S3_BUCKET_NAME, mapping)\n",
    "\n",
    "target: List[str] = []  # this list will contain our actual tensors (as N-dimensional numpy arrays)\n",
    "images: List[np.ndarray] = []  # this list will contain our classes (male or female)\n",
    "\n",
    "for filename, gender in mappings_dict.items():\n",
    "\n",
    "    if \"-14\" in filename or \"-10\" in filename:  # these images are blurry or obscured\n",
    "        continue\n",
    "\n",
    "    local_filename: str = os.path.join(cwd, LOCAL_IMAGES_FOLDER, filename)\n",
    "    try:\n",
    "        if not os.path.isfile(local_filename):  # if file does not exist locally\n",
    "            print(f\"Downloading {filename}, saving as {local_filename}\")\n",
    "            s3.download_file(S3_BUCKET_NAME, filename, local_filename)\n",
    "        else:\n",
    "            logging.debug(f\"Found a local copy of {local_filename}\")\n",
    "\n",
    "        # use the Keras image API to load in an image\n",
    "        img = load_img(local_filename)\n",
    "        #print(\"Resizing\")\n",
    "        img = img.resize((HEIGHT, WIDTH))\n",
    "        #img = img.convert('L')  # convert o gray scale\n",
    "        # report details about the image\n",
    "        images.append(np.array(img))\n",
    "        target.append(gender)\n",
    "        if len(images) == IMAGE_LIMIT:\n",
    "            print(\"Breaking after reaching image limit.\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"{e}:Error downloading {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Target to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding target vector (2396,) -> (2396, 2)\n",
      "There are 2 classes to predict.\n"
     ]
    }
   ],
   "source": [
    "binary_target = np.array(list(map(lambda gender: 0 if gender == 'male' else 1, target)))\n",
    "encoded_target = to_categorical(binary_target)\n",
    "\n",
    "print(f\"One-hot encoding target vector {binary_target.shape} -> {encoded_target.shape}\")\n",
    "NUM_CLASSSES = encoded_target.shape[1]\n",
    "print(f\"There are {NUM_CLASSSES} classes to predict.\")\n",
    "\n",
    "indices = np.linspace(0, len(binary_target) - 1, len(binary_target))\n",
    "validation_indices = np.random.choice(indices, size=int(len(binary_target) * 0.3), replace=False).astype(int)\n",
    "training_indices = set(indices).difference(set(validation_indices))\n",
    "training_indices = np.array(list(training_indices)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Neural Network VGG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "_KERAS_BACKEND = keras.backend\n",
    "_KERAS_LAYERS = keras.layers\n",
    "_KERAS_MODELS = keras.models\n",
    "_KERAS_UTILS = keras.utils\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                'releases/download/v0.1/'\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "WEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n",
    "                       'releases/download/v0.1/'\n",
    "                       'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "def get_submodules_from_kwargs(kwargs):\n",
    "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
    "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
    "    models = kwargs.get('models', _KERAS_MODELS)\n",
    "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
    "    for key in kwargs.keys():\n",
    "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
    "            raise TypeError('Invalid keyword argument: %s', key)\n",
    "    return backend, layers, models, utils\n",
    "\n",
    "def _obtain_input_shape(input_shape,\n",
    "                        default_size,\n",
    "                        min_size,\n",
    "                        data_format,\n",
    "                        require_flatten,\n",
    "                        weights=None):\n",
    "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
    "    # Arguments\n",
    "        input_shape: Either None (will return the default network input shape),\n",
    "            or a user-provided shape to be validated.\n",
    "        default_size: Default input width/height for the model.\n",
    "        min_size: Minimum input width/height accepted by the model.\n",
    "        data_format: Image data format to use.\n",
    "        require_flatten: Whether the model is expected to\n",
    "            be linked to a classifier via a Flatten layer.\n",
    "        weights: One of `None` (random initialization)\n",
    "            or 'imagenet' (pre-training on ImageNet).\n",
    "            If weights='imagenet' input channels must be equal to 3.\n",
    "    # Returns\n",
    "        An integer shape tuple (may include None entries).\n",
    "    # Raises\n",
    "        ValueError: In case of invalid argument values.\n",
    "    \"\"\"\n",
    "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
    "        if data_format == 'channels_first':\n",
    "            if input_shape[0] not in {1, 3}:\n",
    "                warnings.warn(\n",
    "                    'This model usually expects 1 or 3 input channels. '\n",
    "                    'However, it was passed an input_shape with ' +\n",
    "                    str(input_shape[0]) + ' input channels.')\n",
    "            default_shape = (input_shape[0], default_size, default_size)\n",
    "        else:\n",
    "            if input_shape[-1] not in {1, 3}:\n",
    "                warnings.warn(\n",
    "                    'This model usually expects 1 or 3 input channels. '\n",
    "                    'However, it was passed an input_shape with ' +\n",
    "                    str(input_shape[-1]) + ' input channels.')\n",
    "            default_shape = (default_size, default_size, input_shape[-1])\n",
    "    else:\n",
    "        if data_format == 'channels_first':\n",
    "            default_shape = (3, default_size, default_size)\n",
    "        else:\n",
    "            default_shape = (default_size, default_size, 3)\n",
    "    if weights == 'imagenet' and require_flatten:\n",
    "        if input_shape is not None:\n",
    "            if input_shape != default_shape:\n",
    "                raise ValueError('When setting `include_top=True` '\n",
    "                                 'and loading `imagenet` weights, '\n",
    "                                 '`input_shape` should be ' +\n",
    "                                 str(default_shape) + '.')\n",
    "        return default_shape\n",
    "    if input_shape:\n",
    "        if data_format == 'channels_first':\n",
    "            if input_shape is not None:\n",
    "                if len(input_shape) != 3:\n",
    "                    raise ValueError(\n",
    "                        '`input_shape` must be a tuple of three integers.')\n",
    "                if input_shape[0] != 3 and weights == 'imagenet':\n",
    "                    raise ValueError('The input must have 3 channels; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
    "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
    "                    raise ValueError('Input size must be at least ' +\n",
    "                                     str(min_size) + 'x' + str(min_size) +\n",
    "                                     '; got `input_shape=' +\n",
    "                                     str(input_shape) + '`')\n",
    "        else:\n",
    "            if input_shape is not None:\n",
    "                if len(input_shape) != 3:\n",
    "                    raise ValueError(\n",
    "                        '`input_shape` must be a tuple of three integers.')\n",
    "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
    "                    raise ValueError('The input must have 3 channels; got '\n",
    "                                     '`input_shape=' + str(input_shape) + '`')\n",
    "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
    "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
    "                    raise ValueError('Input size must be at least ' +\n",
    "                                     str(min_size) + 'x' + str(min_size) +\n",
    "                                     '; got `input_shape=' +\n",
    "                                     str(input_shape) + '`')\n",
    "    else:\n",
    "        if require_flatten:\n",
    "            input_shape = default_shape\n",
    "        else:\n",
    "            if data_format == 'channels_first':\n",
    "                input_shape = (3, None, None)\n",
    "            else:\n",
    "                input_shape = (None, None, 3)\n",
    "    if require_flatten:\n",
    "        if None in input_shape:\n",
    "            raise ValueError('If `include_top` is True, '\n",
    "                             'you should specify a static `input_shape`. '\n",
    "                             'Got `input_shape=' + str(input_shape) + '`')\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Augmented Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _conv_layer(filters, kernel_size, strides=(1, 1), padding='same', name=None):\n",
    "    return Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
    "                  use_bias=True, kernel_initializer='he_normal', name=name)\n",
    "\n",
    "\n",
    "def _normalize_depth_vars(depth_k, depth_v, filters):\n",
    "    \"\"\"\n",
    "    Accepts depth_k and depth_v as either floats or integers\n",
    "    and normalizes them to integers.\n",
    "    Args:\n",
    "        depth_k: float or int.\n",
    "        depth_v: float or int.\n",
    "        filters: number of output filters.\n",
    "    Returns:\n",
    "        depth_k, depth_v as integers.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(depth_k) == float:\n",
    "        depth_k = int(filters * depth_k)\n",
    "    else:\n",
    "        depth_k = int(depth_k)\n",
    "\n",
    "    if type(depth_v) == float:\n",
    "        depth_v = int(filters * depth_v)\n",
    "    else:\n",
    "        depth_v = int(depth_v)\n",
    "\n",
    "    return depth_k, depth_v\n",
    "\n",
    "\n",
    "class AttentionAugmentation2D(Layer):\n",
    "\n",
    "    def __init__(self, depth_k, depth_v, num_heads, relative=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Applies attention augmentation on a convolutional layer\n",
    "        output.\n",
    "        Args:\n",
    "            depth_k: float or int. Number of filters for k.\n",
    "            Computes the number of filters for `v`.\n",
    "            If passed as float, computed as `filters * depth_k`.\n",
    "        depth_v: float or int. Number of filters for v.\n",
    "            Computes the number of filters for `k`.\n",
    "            If passed as float, computed as `filters * depth_v`.\n",
    "        num_heads: int. Number of attention heads.\n",
    "            Must be set such that `depth_k // num_heads` is > 0.\n",
    "        relative: bool, whether to use relative encodings.\n",
    "        Raises:\n",
    "            ValueError: if depth_v or depth_k is not divisible by\n",
    "                num_heads.\n",
    "        Returns:\n",
    "            Output tensor of shape\n",
    "            -   [Batch, Height, Width, Depth_V] if\n",
    "                channels_last data format.\n",
    "            -   [Batch, Depth_V, Height, Width] if\n",
    "                channels_first data format.\n",
    "        \"\"\"\n",
    "        super(AttentionAugmentation2D, self).__init__(**kwargs)\n",
    "\n",
    "        if depth_k % num_heads != 0:\n",
    "            raise ValueError('`depth_k` (%d) is not divisible by `num_heads` (%d)' % (\n",
    "                depth_k, num_heads))\n",
    "\n",
    "        if depth_v % num_heads != 0:\n",
    "            raise ValueError('`depth_v` (%d) is not divisible by `num_heads` (%d)' % (\n",
    "                depth_v, num_heads))\n",
    "\n",
    "        if depth_k // num_heads < 1.:\n",
    "            raise ValueError('depth_k / num_heads cannot be less than 1 ! '\n",
    "                             'Given depth_k = %d, num_heads = %d' % (\n",
    "                             depth_k, num_heads))\n",
    "\n",
    "        if depth_v // num_heads < 1.:\n",
    "            raise ValueError('depth_v / num_heads cannot be less than 1 ! '\n",
    "                             'Given depth_v = %d, num_heads = %d' % (\n",
    "                                 depth_v, num_heads))\n",
    "\n",
    "        self.depth_k = depth_k\n",
    "        self.depth_v = depth_v\n",
    "        self.num_heads = num_heads\n",
    "        self.relative = relative\n",
    "\n",
    "        self.axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._shape = input_shape\n",
    "\n",
    "        # normalize the format of depth_v and depth_k\n",
    "        self.depth_k, self.depth_v = _normalize_depth_vars(self.depth_k, self.depth_v,\n",
    "                                                           input_shape)\n",
    "\n",
    "        if self.axis == 1:\n",
    "            _, channels, height, width = input_shape\n",
    "        else:\n",
    "            _, height, width, channels = input_shape\n",
    "\n",
    "        if self.relative:\n",
    "            dk_per_head = self.depth_k // self.num_heads\n",
    "\n",
    "            if dk_per_head == 0:\n",
    "                print('dk per head', dk_per_head)\n",
    "\n",
    "            self.key_relative_w = self.add_weight('key_rel_w',\n",
    "                                                  shape=[2 * width - 1, dk_per_head],\n",
    "                                                  initializer=initializers.RandomNormal(\n",
    "                                                      stddev=dk_per_head ** -0.5))\n",
    "\n",
    "            self.key_relative_h = self.add_weight('key_rel_h',\n",
    "                                                  shape=[2 * height - 1, dk_per_head],\n",
    "                                                  initializer=initializers.RandomNormal(\n",
    "                                                      stddev=dk_per_head ** -0.5))\n",
    "\n",
    "        else:\n",
    "            self.key_relative_w = None\n",
    "            self.key_relative_h = None\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.axis == 1:\n",
    "            # If channels first, force it to be channels last for these ops\n",
    "            inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])\n",
    "\n",
    "        q, k, v = tf.split(inputs, [self.depth_k, self.depth_k, self.depth_v], axis=-1)\n",
    "\n",
    "        q = self.split_heads_2d(q)\n",
    "        k = self.split_heads_2d(k)\n",
    "        v = self.split_heads_2d(v)\n",
    "\n",
    "        # scale query\n",
    "        depth_k_heads = self.depth_k / self.num_heads\n",
    "        q *= (depth_k_heads ** -0.5)\n",
    "\n",
    "        # [Batch, num_heads, height * width, depth_k or depth_v] if axis == -1\n",
    "        qk_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_k // self.num_heads]\n",
    "        v_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_v // self.num_heads]\n",
    "        flat_q = K.reshape(q, K.stack(qk_shape))\n",
    "        flat_k = K.reshape(k, K.stack(qk_shape))\n",
    "        flat_v = K.reshape(v, K.stack(v_shape))\n",
    "\n",
    "        # [Batch, num_heads, HW, HW]\n",
    "        logits = tf.matmul(flat_q, flat_k, transpose_b=True)\n",
    "\n",
    "        # Apply relative encodings\n",
    "        if self.relative:\n",
    "            h_rel_logits, w_rel_logits = self.relative_logits(q)\n",
    "            logits += h_rel_logits\n",
    "            logits += w_rel_logits\n",
    "\n",
    "        weights = K.softmax(logits, axis=-1)\n",
    "        attn_out = tf.matmul(weights, flat_v)\n",
    "\n",
    "        attn_out_shape = [self._batch, self.num_heads, self._height, self._width, self.depth_v // self.num_heads]\n",
    "        attn_out_shape = K.stack(attn_out_shape)\n",
    "        attn_out = K.reshape(attn_out, attn_out_shape)\n",
    "        attn_out = self.combine_heads_2d(attn_out)\n",
    "        # [batch, height, width, depth_v]\n",
    "\n",
    "        if self.axis == 1:\n",
    "            # return to [batch, depth_v, height, width] for channels first\n",
    "            attn_out = K.permute_dimensions(attn_out, [0, 3, 1, 2])\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[self.axis] = self.depth_v\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def split_heads_2d(self, ip):\n",
    "        tensor_shape = K.shape(ip)\n",
    "\n",
    "        # batch, height, width, channels for axis = -1\n",
    "        tensor_shape = [tensor_shape[i] for i in range(len(self._shape))]\n",
    "\n",
    "        batch = tensor_shape[0]\n",
    "        height = tensor_shape[1]\n",
    "        width = tensor_shape[2]\n",
    "        channels = tensor_shape[3]\n",
    "\n",
    "        # Save the spatial tensor dimensions\n",
    "        self._batch = batch\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "\n",
    "        ret_shape = K.stack([batch, height, width,  self.num_heads, channels // self.num_heads])\n",
    "        split = K.reshape(ip, ret_shape)\n",
    "        transpose_axes = (0, 3, 1, 2, 4)\n",
    "        split = K.permute_dimensions(split, transpose_axes)\n",
    "\n",
    "        return split\n",
    "\n",
    "    def relative_logits(self, q):\n",
    "        shape = K.shape(q)\n",
    "        # [batch, num_heads, H, W, depth_v]\n",
    "        shape = [shape[i] for i in range(5)]\n",
    "\n",
    "        height = shape[2]\n",
    "        width = shape[3]\n",
    "\n",
    "        rel_logits_w = self.relative_logits_1d(q, self.key_relative_w, height, width,\n",
    "                                               transpose_mask=[0, 1, 2, 4, 3, 5])\n",
    "\n",
    "        rel_logits_h = self.relative_logits_1d(\n",
    "            K.permute_dimensions(q, [0, 1, 3, 2, 4]),\n",
    "            self.key_relative_h, width, height,\n",
    "            transpose_mask=[0, 1, 4, 2, 5, 3])\n",
    "\n",
    "        return rel_logits_h, rel_logits_w\n",
    "\n",
    "    def relative_logits_1d(self, q, rel_k, H, W, transpose_mask):\n",
    "        rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads * H, W, 2 * W - 1])\n",
    "        rel_logits = self.rel_to_abs(rel_logits)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H, W, W])\n",
    "        rel_logits = K.expand_dims(rel_logits, axis=3)\n",
    "        rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])\n",
    "        rel_logits = K.permute_dimensions(rel_logits, transpose_mask)\n",
    "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H * W, H * W])\n",
    "        return rel_logits\n",
    "\n",
    "    def rel_to_abs(self, x):\n",
    "        shape = K.shape(x)\n",
    "        shape = [shape[i] for i in range(3)]\n",
    "        B, Nh, L, = shape\n",
    "        col_pad = K.zeros(K.stack([B, Nh, L, 1]))\n",
    "        x = K.concatenate([x, col_pad], axis=3)\n",
    "        flat_x = K.reshape(x, [B, Nh, L * 2 * L])\n",
    "        flat_pad = K.zeros(K.stack([B, Nh, L - 1]))\n",
    "        flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)\n",
    "        final_x = K.reshape(flat_x_padded, [B, Nh, L + 1, 2 * L - 1])\n",
    "        final_x = final_x[:, :, :L, L - 1:]\n",
    "        return final_x\n",
    "\n",
    "    def combine_heads_2d(self, inputs):\n",
    "        # [batch, num_heads, height, width, depth_v // num_heads]\n",
    "        transposed = K.permute_dimensions(inputs, [0, 2, 3, 1, 4])\n",
    "        # [batch, height, width, num_heads, depth_v // num_heads]\n",
    "        shape = K.shape(transposed)\n",
    "        shape = [shape[i] for i in range(5)]\n",
    "\n",
    "        a, b = shape[-2:]\n",
    "        ret_shape = K.stack(shape[:-2] + [a * b])\n",
    "        # [batch, height, width, depth_v]\n",
    "        return K.reshape(transposed, ret_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'depth_k': self.depth_k,\n",
    "            'depth_v': self.depth_v,\n",
    "            'num_heads': self.num_heads,\n",
    "            'relative': self.relative,\n",
    "        }\n",
    "        base_config = super(AttentionAugmentation2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def augmented_conv2d(ip, filters, kernel_size=(3, 3), strides=(1, 1),\n",
    "                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):\n",
    "    \"\"\"\n",
    "    Builds an Attention Augmented Convolution block.\n",
    "    Args:\n",
    "        ip: keras tensor.\n",
    "        filters: number of output filters.\n",
    "        kernel_size: convolution kernel size.\n",
    "        strides: strides of the convolution.\n",
    "        depth_k: float or int. Number of filters for k.\n",
    "            Computes the number of filters for `v`.\n",
    "            If passed as float, computed as `filters * depth_k`.\n",
    "        depth_v: float or int. Number of filters for v.\n",
    "            Computes the number of filters for `k`.\n",
    "            If passed as float, computed as `filters * depth_v`.\n",
    "        num_heads: int. Number of attention heads.\n",
    "            Must be set such that `depth_k // num_heads` is > 0.\n",
    "        relative_encodings: bool. Whether to use relative\n",
    "            encodings or not.\n",
    "    Returns:\n",
    "        a keras tensor.\n",
    "    \"\"\"\n",
    "    print(f\"Using {num_heads} number of heads\")\n",
    "    # input_shape = K.int_shape(ip)\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)\n",
    "\n",
    "    conv_out = _conv_layer(filters - depth_v, kernel_size, strides)(ip)\n",
    "\n",
    "    # Augmented Attention Block\n",
    "    qkv_conv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(ip)\n",
    "    attn_out = AttentionAugmentation2D(depth_k, depth_v, num_heads, relative_encodings)(qkv_conv)\n",
    "    attn_out = _conv_layer(depth_v, kernel_size=(1, 1))(attn_out)\n",
    "\n",
    "    output = concatenate([conv_out, attn_out], axis=channel_axis)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_VGG16(include_top=False, # switched from True to False\n",
    "          weights='imagenet',\n",
    "          input_tensor=None,\n",
    "          input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=2, # switched from 1000 default classes to 2 classes\n",
    "          **kwargs):\n",
    "    \"\"\"Instantiates the VGG16 architecture.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)`\n",
    "            (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 input channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # Multi-Headed Attention Layer\n",
    "    x = augmented_conv2d(img_input, filters=10, kernel_size=(3, 3), depth_k=0.2, depth_v=0.2,  # dk/v (0.2) * f_out (20) = 4\n",
    "                         num_heads=2, relative_encodings=True)         \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv1')(x)\n",
    "    x = layers.Conv2D(64, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block1_conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(128, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block2_conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv2')(x)\n",
    "    x = layers.Conv2D(256, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block3_conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv1')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv2')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block4_conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv1')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv2')(x)\n",
    "    x = layers.Conv2D(512, (3, 3),\n",
    "                      activation='relu',\n",
    "                      padding='same',\n",
    "                      name='block5_conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "        else:\n",
    "            weights_path = keras_utils.get_file(\n",
    "                'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "        model.load_weights(weights_path)\n",
    "        if backend.backend() == 'theano':\n",
    "            keras_utils.convert_all_kernels_in_model(model)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 number of heads\n",
      "(?, 120, 160, 12)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 120, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 120, 160, 6)  24          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_augmentation2d_4 (Att (None, 120, 160, 2)  1116        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 120, 160, 10) 280         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 120, 160, 2)  6           attention_augmentation2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 120, 160, 12) 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 240, 320, 3)  0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   (None, 512)          14714688    reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          65664       vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            258         batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 14,782,548\n",
      "Trainable params: 7,147,028\n",
      "Non-trainable params: 7,635,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Reshape\n",
    "\n",
    "input_dims = (WIDTH, HEIGHT, 3)\n",
    "inputs = Input(shape=input_dims)\n",
    "x = augmented_conv2d(inputs, filters=12, kernel_size=(3, 3), depth_k=0.2, depth_v=0.2,  # dk/v (0.2) * f_out (20) = 4\n",
    "                         num_heads=1, relative_encodings=True)\n",
    "print(x.shape)\n",
    "vgg=VGG16(include_top=False, pooling='avg', weights='imagenet',input_shape=(WIDTH, HEIGHT, 3))\n",
    "for layer in vgg.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Reshape((WIDTH * 2, HEIGHT * 2, 3))(x)\n",
    "x = vgg(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=0.0000005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (1678, 60, 80, 3) to (1678, 60, 80, 3, 1)\n",
      "Expanding shape from (718, 60, 80, 3) to (718, 60, 80, 3, 1)\n",
      "Expanding shape from (2396, 60, 80, 3) to (2396, 60, 80, 3, 1)\n",
      "The shape of X_train_expanded is (1678, 60, 80, 3, 1)\n",
      "The shape of X_test_expanded is (718, 60, 80, 3, 1)\n",
      "The shape of X_train is (1678, 60, 80, 3)\n",
      "The shape of y_train is (1678, 2)\n",
      "The shape of X_test is (718, 60, 80, 3)\n",
      "The shape of y_test is (718, 2) - some example targets:\n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "images: np.ndarray = np.array(images)\n",
    "X_train = images[training_indices]\n",
    "y_train = binary_target[training_indices]\n",
    "y_train = encoded_target[training_indices]\n",
    "X_test = images[validation_indices]\n",
    "y_test = binary_target[validation_indices]\n",
    "y_test = encoded_target[validation_indices]\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)\n",
    "images_expanded = expand_tensor_shape(images)\n",
    "\n",
    "print(f\"The shape of X_train_expanded is {X_train_expanded.shape}\")\n",
    "print(f\"The shape of X_test_expanded is {X_test_expanded.shape}\")\n",
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape} - some example targets:\\n {y_test[:5]}\")\n",
    "\n",
    "input_dims = (WIDTH, HEIGHT, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 120, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 120, 160, 6)  24          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_augmentation2d_3 (Att (None, 120, 160, 2)  1116        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 120, 160, 10) 280         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 120, 160, 2)  6           attention_augmentation2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 120, 160, 12) 0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 240, 320, 3)  0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   (None, 512)          14714688    reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          65664       vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 14,782,548\n",
      "Trainable params: 7,147,028\n",
      "Non-trainable params: 7,635,520\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1677 samples, validate on 719 samples\n",
      "Epoch 1/20\n",
      " 152/1677 [=>............................] - ETA: 27:40 - loss: 0.6931 - acc: 0.7171"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6d0182c84370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msaved_model_relative_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved_models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mha_vgg_faces.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_relative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.fit(images, encoded_target, epochs=20, batch_size=1, validation_split=0.3, verbose=1, shuffle=True)\n",
    "saved_model_relative_path: str = os.path.join(\"saved_models\", \"mha_vgg_faces.h5\")\n",
    "model.save(saved_model_relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_relative_path: str = os.path.join(\"saved_models\", \"mha_vgg_faces.h5\")\n",
    "from keras.models import load_model\n",
    "warnings.filterwarnings('ignore')\n",
    "model = load_model(saved_model_relative_path,  custom_objects={'AttentionAugmentation2D': AttentionAugmentation2D})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,1,60,80,60,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node attention_augmentation2d_23_1/transpose_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dense_17_1/Softmax/_3033]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,1,60,80,60,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node attention_augmentation2d_23_1/transpose_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c8aaa6aec41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,1,60,80,60,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node attention_augmentation2d_23_1/transpose_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[dense_17_1/Softmax/_3033]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,1,60,80,60,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node attention_augmentation2d_23_1/transpose_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "model.predict(images[0:500]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1196., 1200.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_target.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_augmentation_output = model.get_layer('concatenate_19').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_output = K.function([model.layers[0].input], [attention_augmentation_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT = 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = get_output([images[SUBJECT].reshape((1,WIDTH,HEIGHT,3))])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps = np.mean(attention_output[0], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[SUBJECT])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.12654 , 13.631502, 13.711227, ..., 13.982691, 14.008222,\n",
       "        15.586621],\n",
       "       [17.592712, 15.879021, 16.10599 , ..., 16.366304, 16.276978,\n",
       "        18.714912],\n",
       "       [17.091194, 15.673492, 15.562767, ..., 16.026136, 16.102804,\n",
       "        18.677584],\n",
       "       ...,\n",
       "       [17.800657, 16.503181, 16.791052, ..., 17.160728, 17.131966,\n",
       "        19.548456],\n",
       "       [17.390882, 16.97164 , 16.52903 , ..., 17.087915, 16.947248,\n",
       "        19.348763],\n",
       "       [20.064861, 20.033257, 20.44158 , ..., 20.31806 , 20.440655,\n",
       "        28.430685]], dtype=float32)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_maps * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Heatmap\n",
    "\n",
    "Within the `make_heatmap` function, there is a binary threshold that zeroes out the most insignificant attention spots. By default, that value is `20`. However, it will differ from subject to subject - some subjects feature extremely high attention regions, while others see significantly less attention output values in general. Therefore, for future iterations of this model, it may make sense to perform some form of min-max scaling, rather than hardcode in some binary threshold factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap is shape (60, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "def make_heatmap(cam: np.ndarray)-> np.ndarray:\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam < 19)] = 0\n",
    "    print(f\"Heatmap is shape {heatmap.shape}\")\n",
    "    return heatmap\n",
    "\n",
    "heatmap = make_heatmap(attention_maps * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Male (53.0%)')"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = model.predict(images[SUBJECT].reshape((1,60,80,3)))\n",
    "predicted_gender = \"Male\" if prediction.argmax() == 0 else \"Female\"\n",
    "confidence = np.round(prediction,2) * 100\n",
    "final_image = heatmap + images[SUBJECT]\n",
    "plt.imshow(np.array(final_image,np.int32))\n",
    "plt.title(f\"Predicted {predicted_gender} ({round(confidence[0][prediction.argmax()],2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36] *",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
